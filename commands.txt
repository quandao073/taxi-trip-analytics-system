redis-cli -h redis-sentinel.bigdata.svc.cluster.local -p 26379 -a quanda

SENTINEL get-master-addr-by-name mymaster

redis-cli -h redis-sentinel-node-0.redis-sentinel-headless.bigdata.svc.cluster.local -p 6379 -a quanda


volumeMounts:
            - mountPath: /run/src/main/resources/application.properties
              name: ecommerce-backend-application-properties-config-volume
              subPath: application.properties


volumes:
        - configMap:
            defaultMode: 420
            name: ecommerce-backend-application-properties-configmap
          name: ecommerce-backend-application-properties-config-volume



---------------------------------------------------------------------------------------------------------------------------------

Kafka

helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
helm repo update


# cài nfs provisioner tu dong tao pvc

helm install nfs-kafka-storage nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
  --namespace kube-system \
  --set nfs.server=192.168.164.206 \
  --set nfs.path=/A/kafka \
  --set storageClass.name=nfs-kafka-storage \
  --set storageClass.defaultClass=false \
  --set storageClass.accessModes={ReadWriteOnce} \
  --set storageClass.reclaimPolicy=Delete

# lenh update thay doi sang Retain (Giu lai du lieu sau khi xoa pvc)

helm upgrade nfs-kafka-storage nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
  --namespace kube-system \
  --set nfs.server=192.168.164.206 \
  --set nfs.path=/A/kafka \
  --set storageClass.name=nfs-kafka-storage \
  --set storageClass.defaultClass=false \
  --set storageClass.accessModes={ReadWriteOnce} \
  --set storageClass.reclaimPolicy=Retain

----------------------------------------------------------------------------------------------------------------------------------
1. Install Airflow

helm install nfs-airflow-storage nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
  --namespace kube-system \
  --set nfs.server=192.168.164.206 \
  --set nfs.path=/A/airflow \
  --set storageClass.name=nfs-airflow-storage \
  --set storageClass.defaultClass=false \
  --set storageClass.accessModes={ReadWriteMany} \
  --set storageClass.reclaimPolicy=Retain

helm repo add apache-airflow https://airflow.apache.org

cd airflow/
helm install airflow apache-airflow/airflow -n bigdata -f values.yaml

helm upgrade airflow apache-airflow/airflow -n bigdata -f values.yaml

helm uninstall airflow -n bigdata

chmod -R 777 /A/airflow/
chown -R nobody:nogroup /A/airflow/

spark-submit /opt/airflow/code/write_data.py 2024 11

spark-submit --packages org.postgresql:postgresql:42.7.5 /opt/airflow/code/quick_analytics_job.py


airflow dags trigger -e 2024-12-01 batch_processing_dag

airflow dags trigger -e "2021-11-01T01:30:00" batch_processing_dag

airflow dags list-runs -d batch_processing_dag

airflow dags delete-run --dag-id batch_processing_dag --run-id manual__2021-11-02T00:00:00+00:00

----------------------------------------------------------------------------------------------------------------------------------

- copy code to dags and code folder

2. Install Hadoop
helm repo add pfisterer-hadoop https://pfisterer.github.io/apache-hadoop-helm/

cd hadoop/
helm install hadoop pfisterer-hadoop/hadoop --namespace bigdata -f values.yaml

helm upgrade hadoop pfisterer-hadoop/hadoop --namespace bigdata -f values.yaml

helm uninstall hadoop --namespace bigdata


helm install nfs-hadoop-storage nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
  --namespace kube-system \
  --set nfs.server=192.168.164.206 \
  --set nfs.path=/B/hadoop \
  --set storageClass.name=nfs-hadoop-storage \
  --set storageClass.defaultClass=false \
  --set storageClass.accessModes={ReadWriteOnce} \
  --set storageClass.reclaimPolicy=Retain

----------------------------------------------------------------------------------------------------------------------------------
3. Redis
helm repo add bitnami https://charts.bitnami.com/bitnami

cd redis/

helm install redis bitnami/redis --values values.yaml --namespace bigdata

helm uninstall redis --namespace bigdata

redis-cli -a quanda

SUBSCRIBE realtime-trip-channel
	
SET total_trips 12345
TTL total_trips

----------------------------------------------------------------------------------------------------------------------------------
4. Spark Cluster

cd spark/
helm install spark bitnami/spark --values values.yaml --namespace bigdata
helm upgrade spark bitnami/spark -f values.yaml -n bigdata

helm uninstall spark -n bigdata

spark://spark-master-svc.bigdata.svc.cluster.local:7077


----------------------------------------------------------------------------------------------------------------------------------

5. PostgreSQL

helm repo add bitnami https://charts.bitnami.com/bitnami

cd postgreSQL/
helm install postgresql-db bitnami/postgresql --values values.yaml --namespace bigdata

helm upgrade postgresql-db bitnami/postgresql -f values.yaml -n bigdata


helm uninstall postgresql-db -n bigdata


psql -U quanda -d taxi_trip_db

doi recover du lieu roi lam tiep

----------------------------------------------------------------------------------------------------------------------------------

6. Superset

cd superset/
helm install superset bitnami/superset --values values.yaml --namespace bigdata

helm uninstall superset -n bigdata

superset.bigdata.svc.cluster.local

connect to PostgreSQL: 
postgresql+psycopg2://quanda:quanda@postgresql-db.bigdata.svc.cluster.local:5432/taxi_trip_db

helm install nfs-superset-storage nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
  --namespace kube-system \
  --set nfs.server=192.168.164.206 \
  --set nfs.path=/A/superset \
  --set storageClass.name=nfs-superset-storage \
  --set storageClass.defaultClass=false \
  --set storageClass.accessModes={ReadWriteMany} \
  --set storageClass.reclaimPolicy=Delete


----------------------------------------------------------------------------------------------------------------------------------


kubeadm join 192.168.164.201:6443 --token v67jwx.wq9o3sifneya0fed \
        --discovery-token-ca-cert-hash sha256:ee2b784c93b1561b10ddbe813ba212fa55cf931054868f741677f5556563ae09


----------------------------------------------------------------------------------------------------------------------------------
--- Velero ---

velero backup create k8s-bidata-v1 --include-namespaces bigdata --snapshot-volumes=false
velero restore create quanda-k8s-v1 --from-backup quanda-k8s-v1 --include-namespaces bigdata

velero backup delete k8s-bidata-v1
velero backup create all-cluster-v1 --include-namespaces '*' --snapshot-volumes=false

velero schedule create daily-cluster-backup --schedule="0 0 * * *" --include-namespaces '*'
velero schedule get

- Slide em không thay đổi nhiều so với lần trước, em sẽ thay đổi cách mình trình bày.
- Tên đề tài của em nghe có vẻ rất rộng, vì vậy khi trình bày phần giới thiệu em sẽ giới hạn luôn lại phạm vi: "Trong phạm vi thử nghiệm với dữ liệu các chuyến taxi..., em đưa ra giải pháp.." để tập trung vào bài toán taxi. Ý tưởng của em là hệ thống có khả năng linh hoạt trong việc thay đổi mã nguồn hay triển khai các công nghệ khác nhau, việc nói chung chung là "hệ thống ... dữ liệu lớn" dễ gây hiểu nhầm, có thể sẽ bị hỏi khó phần này
- Theo nhận xét của thầy Đức, em cần phải tập trung vào giải quyết bài toán thay vì triển khai như thế nào. Vì vậy em sẽ trình bày chi tiết toàn bộ những việc mình làm trong phần 2, với bộ dữ liệu thử nghiệm, và phần kết quả thực nghiệm sẽ bổ sung thêm 1 chút. Phần triển khai chỉ trình bày cấu trúc và cách triển khai, lướt nhanh qua các hình ảnh kết quả.
- Slide của em khá dài (40 trang) nhưng chủ yếu là hình ảnh kết quả, em sẽ lướt nhanh những phần này. Em dự định trình bày trong khoảng 11-12 phút.
- Nếu bị hỏi case study của em là về big data thì em đang chưa biết cách xử lý, vì em cũng chưa được tiếp xúc với "big data" thực sự bao giờ, và dữ liệu em kiếm được là dữ liệu em thấy có dung lượng lớn nhất rồi. Nhưng với đồ án này, thì em nghĩ mình sẽ nói về các công cụ có khả năng mở rộng tốt, tăng khả năng tính toán vô hạn miễn là có đủ tài nguyên.